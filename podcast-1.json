{"podcast_details": {"podcast_title": "The Future of Everything", "episode_title": "Best of: How computer chips get speedier through specialization", "episode_image": "https://image.simplecastcdn.com/images/8a55b2a3-28ac-463f-8967-50f663d7d93a/72d23192-3797-4318-8771-5b7db3819aa3/3000x3000/2319-stanford-engineering-podcasts-assets-m1-artwork-1.jpg?aid=rss_feed", "episode_transcript": " Hey everyone, it's Russ Altman here from the Future of Everything. Thanks for tuning in. Today we're rerunning a conversation I had in 2021 with Priya Raina, a professor of electrical engineering at Stanford University. Priya discusses the slowing pace of progress in chip efficiency and how she sees a future where chip makers will need to shift away from general purpose chips to task specific chips. Before we jump into this episode, a reminder to please rate and review it on your favorite podcast app. It'll help us grow and it'll help other listeners figure out what this is all about. Computer chips have been on people's minds recently, mostly because there are problems in the supply chain that are challenging the auto industry. There are some types of computer chips that are critical for new cars. So even if you have your engine and your chassis and your exterior and interior, if you don't have those computer chips, you're not going to really be able to use your new car. I was reading that cars require between 500 and 2000 separate computer chips that encode the logic for everything, the air conditioning, the entertainment system, cruise control, door locks. By the way, this is one of the reasons I love my 1967 Ford Galaxy 500, which although it does have some electrical systems, it definitely doesn't have a computer or any computer chips. But life has gotten much better in the last 50 years. So computer chips are these tiny devices smaller than a postage stamp or so that contain electronic circuits made of silicon. You've heard of Silicon Valley and they have thousands of components, including transistors and many other electrical components. With a chip, you can encode the logic required to do math, to make a radio, a cell phone, a television or refrigerator or any other device turn on and off and respond to input commands or sensors in order to create different types of outputs. Now it is hard to build any device, even toasters, without embedding a chip that provides useful features. And there is a huge market for new chips that can provide nifty new features and capabilities quickly and cheaply. And there's great competition between companies to have devices that perform the best, efficient, effective, easy to use, reliable, robust, or whatever you want. Professor Priyanka Raina is a professor of electrical engineering at Stanford. Priya, you have said that you want to make chip design more accessible. Now I had not thought about computer chips as a matter of justice, but maybe it is. So can you explain what's the current process for creating these chips? What is the problems with that process? And where do you see an opportunity to make things more accessible? Russ, thanks for the great introduction. That was a fantastic overview of the importance of computer chips in the everyday world. You use them everywhere. And as you mentioned, they are built out of transistors and more precisely, silicon transistors, silicon being the semiconductor. That's why they're called semiconductor chips. Now it's worth noting the progress that we have made in silicon transistor technology over the years. This is sometimes referred to as Moore's law, which basically states that the number of transistors in a chip double almost every two years. And so in the same chip area, you can get double the number of transistors. And what that means is that in the same area, you can embed more functionality. Now it's not only that. As these transistors get smaller and smaller, they also get faster and they also get lower power. So for the same function, you can do it quicker. You can do it with lower battery consumption and so on. And this progression has happened over the last several decades where we are improving in computing performance. Now what has happened is more recently in the last couple of years, this cadence has slowed down. And so now computer chips are not getting faster and lower power at the same rate as it had been for several, several decades. And because of that, this raises new challenges. So one important challenge is that for so many years, we've been relying on an architecture called a CPU, central processing unit, which is basically your general purpose computer to do stuff. Now that general purpose computer has a few instructions, like load something from the memory, do some compute on it, like add, multiply, store it back in the memory and so on. So it's a very simple architecture. And with those couple of instructions, you can basically execute any type of program. So you can basically code in some high level language, see Python, anything, and then you can run that program. So these computers can basically run anything. Now the problem is when you have a computer capable of running anything, it's not as efficient. And so far with Moore's law, computers were just getting faster. These CPUs, these general purpose programmable processors were getting faster. And so no one really thought about changing that architecture. So the reason it's not, just so I can understand, the reason it's not as efficient is because you can do so many things, there's a lot of general purpose capabilities that might not be used for a particular application, and yet you're still paying for that capability. And you kind of want to, to be most efficient, you would shed everything you don't need and only use the parts of the circuit that are useful. Is that? Exactly. That's exactly the case. So more recently, there has been a trend towards building these more specialized chips. There are several names for it. They're called application specific integrated circuits or ASICs, also called accelerators or hardware accelerators. And so these accelerators are also chips. They're just not as flexible as your processors. Which I'm guessing is fine, because if you buy it for your toaster, you just need it to know about toasting and you don't need it to be able to do like college calculus or anything like that. Exactly. Exactly. So the first accelerators that became very popular were GPUs. So GPUs are actually graphics processing unit. They were actually accelerators specialized for graphics, right? So that the rendering on your computer screens happens very fast with low energy and so on. With time, people have added more flexibility to GPUs to be able to use them for other things and get high performance. Now with this slowdown in technology, now people are bringing more and more focus to creating these specialized accelerators, right? And that's what my research is about. And that is also related to the class that I'm teaching, which is how to make it easier for people to design these specialized chips. And they are going to be necessary if we want to kind of keep improving the performance of computing systems advocated. So I guess in order to understand the opportunities for the future, tell me how, I have no idea how long it takes from the time you say, I need a chip to the time that you can deliver a chip for your purpose. So can you take us through what is the old fashioned way of building chips? I think we should start by looking at a little bit of a history. So when IC started or integrated circuit started, people used to hand design every transistor and hand lay it out, right? But because there were some tens or 20 transistors, right? You could do that scalably, right? As we, with Moore's law, as we got more and more transistors on a chip, it became more and more impossible to hand design everything, right? Chips these days have billions of transistors. So it's impossible to think about handling out each transistor and creating that circuit. So in parallel to the chip technology scaling revolution, there was a parallel revolution, which is called EDA or electronic design automation. So what electronic design automation means is that instead of physically figuring out where each transistor goes, you raise the level of abstraction for designing. So you write code in a higher level language, and then there are tools that can basically compile that code and generate your design for the chip, right? So you're not doing it manually at all. You are now encoding what you want to do. So for example, in this high level language, if I write a plus, there is some tool that will take that plus and generate an addition circuit. And then there's another tool that will take that addition circuit and actually lay out the physical transistors that will get fabricated by the foundry. So if I understand what you said, you basically write the code. Okay, so forgive me for using the toaster, but I find it an entertaining example. You write the code for how to control the toaster, and then it figures out how to build a toaster chip by looking at all the steps that you put into your computer software code. Okay, okay. So that sounds pretty good. Does that make it pretty fast? So it does make it pretty fast. However, what has happened is because of this technology shrinking, it has not just shrunk, it has also become more complicated, right? Because now you have very tiny transistors, and they're very finicky, right? And so you have to follow a lot of rules when you're designing these circuits. And all of those rules, since these tools are automatically generating that circuit, all of those rules are encoded in these EDA tools that allow you to generate those circuits. So these tools have become extremely complicated. And if you're a student, it takes you on the order of like several months to get familiar with like using a tool and figuring out all its nuances and things like that. This is the future of everything. I'm Russ Altman. I'm speaking with Professor Priya Raina about the chips, past and present and future. Okay, so that was a key answer, because you said this is now taking months to learn. And even when you learn it, I guess that it's weeks to months to get it to do the things you want it to do. So and even then, the output of these EDA programs is just the design of the chip, right? It's not a physical chip yet. Right. Do things go quickly after that? Or is it once again a set of bottlenecks? So after that, it's not your responsibility, it's responsibility of the fabrication facility. Right. So you hand off this design description to them that has the layout of all the transistors, the billion transistors, and then they take that through the manufacturing process. Right. And that takes several months, maybe two or three months to get fabricated. So if you are working in a university, and you have an idea, and you want to tape out a chip, and get it all the way into the fab and then test it, it'll take you on the order of a year to do all of this process to do the design to verify it, send it to the fab, get it back, test it. And it's on the same order, like companies also take the same sort of order of time, because they are designing much more complicated chips, they have larger teams. So you see, you know, every year or so you get a new product release, which will have a new chip. So that's the cadence at which these things improve. And that's a big deal, because I'm sure, just knowing general life, that you then have to order a bunch of these to make it economically worthwhile. You can't just say, I want two of them. You've just put a year of work in, and if you're a car manufacturer or whatever, you need to buy thousands or millions of them in order to make it worthwhile. So okay, now I understand why agility would be useful. So now can you tell me what are you and your group doing to try to make this process faster? Because I could imagine if the ability to make it like you find a bug, and that's a disaster, because then you have to redo it, or there's no bug, but it's just not as good as it could be. And just through use of the device, you realize that there are opportunities to make the chip better. And if I have to wait a year, I could imagine that that's not as good as if I only had to wait a month or a week. So tell me about the opportunities that your group sees for improving all this. So there are basically two parts to this. All of the agility basically comes from a central principle, which is reuse. The more you reuse from previous generation, the faster your next chip will get taped out. And so even in companies, people don't design chips from scratch. You start from some base platform, and then you make some incremental improvements to it, and then that's your next chip. And the more you can reuse, the faster you can spin. You only want to spend time on things that are very critical to change between one generation to the next. Now there are two aspects to this. One is reuse in the hardware itself, and the other is reuse in the software tool chain. So these chips, they don't just function on their own. The hardware itself is not enough. You need the hardware, but then you need the entire software tool chain on top of it, which makes it do some useful work. So you need the compiler stack, and then you need all of the high level programming models and everything from Python all the way down to your chip. So all of that also takes time, and it's a significant chunk of time. So in... For those who are not familiar, I just want to say Python, we've mentioned Python a couple of times. It's a high level programming language that often you're even seeing students learning this in high school, and it allows them to control a computer through relatively straightforward programming. But in order to be able to write the Python, there has to be the computer behind it that understands the Python and can do all the computations. So excuse my interruption. That was a great introduction. So there's the hardware and then there's software. So I'll address this one by one. So on the hardware side, one of the things that I did not mention so far is that these foundries that do the chip fabrication, because they have spent so many years and so much money into building the technology, that technology is heavily guarded. So the rules for that technology are not openly available. And so if you want to manufacture using that technology, you have to sign some NDAs and you can't disclose any information in that technology and things like that. So because of that, there is very little open source hardware. There's a lot of open source software. Because of this technology component, there's very little open source hardware. Now open source, what that does is that it allows reuse. Someone doesn't have to implement all of these libraries that someone else has written, they can just use it. But that doesn't happen for hardware because of this heavily guarded information. Now recently, what has happened is there's one technology that has been open sourced. It's called Skywater technology. They have open sourced a 130 nanometer process. It's a very old process, but it's open source. Is 130 nanometer, does that refer to the thickness of the wires? So it refers to the length of the gate inside the transistor. It's a very old technology. So for reference, right now we are at like five nanometer and this is 130. So it's 20 times less impressive. So what we have done is now this 130 nanometer technology is openly available and there's a lot of momentum around building an open source, both a repository of open source hardware, as well as a repository of open source EDA tools that can use that technology to produce useful chips. Because the technology is private, the EDA tools are also all closely guarded. So there aren't very many open source EDA tools because they have spent a lot of time in optimizing for those technologies and they don't want to leak out that information. So now we have finally, we have some momentum towards open source hardware and open source EDA tools. And I think that is revolutionary because it is this open source movement that has led to so much progress on the software side. And now if we can replicate that in hardware side, what that encourages is a lot of innovation, right? If there is low barrier to entry because of this existing IP and things that people can reuse, people can innovate more and generate more ideas. And so that's what I'm excited about on the hardware side. Yes. And I do see the revolutionary aspect here because if you're training the students and then if they get hired by a company, they're going to say, well, when I was at the university, I was using these tools. I'm very familiar with them. I've designed several chips with them. And then the companies will have some pressure, presumably, to adopt some of those open source tools rather than have to retrain their workforce. And so I can see now, how do the companies feel about this? So I think companies are like, I don't think they would be open sourcing their stuff anytime soon, but I think if there's enough momentum, they might start contributing some portions to these because, you know, training a workforce for at an older technology, you know, but like it's not that much of a threat, right? Because your fastest processors and GPUs are still going to be in five nanometer. And I don't see that getting open sourced, right? But what this will do is even at 130 nanometer, if you can design a specialized chip, there's a lot of performance gain that you can get just and not everything needs the fastest, most powerful. Exactly. Exactly. I'm sure there's a curve and that they'll be in fine shape for the most compute intensive applications. But for many things that don't need to be at the at the limit, now you're creating a workforce and a set of tools that will allow them to more quickly generate these chips. You gave us this great overview of the old way of doing chips and now this new open source way that will and I can see why it will democratize because it's open source, people can have access to it, they can build on each other's work. And therefore the revision time instead of being a year might turn into months or weeks, which I can imagine is very exciting. But I also could imagine that that new revision time could put pressure on the software that you're developing because in the old model, you didn't have to update your software for a year either because the chips weren't changing. When the underlying chips change more, my guess is that the software has to change. So is that a problem and how are you addressing it? A lot of the time in the actual deployment of chips goes into building up the software tool chain because you can't deploy chips until you have software. And a lot of deployments stall because people don't have the compilers of the software ready. So this is a big issue and this is one of the key problems that we are addressing in research. So as we make chip design more and more agile, can we also make the software more and more agile? Before that you have to pause and think about how people did this in the CPU era, in the general purpose processor era. So even though processors were improving constantly, what didn't change was sort of the instruction set architecture of that processor. So there was a common interface that was static and there was hardware improvements happening below and there were software improvements happening above because they could target the same instruction set architecture. They literally spoke the same language and so it hid from you the details of the hardware. I'm guessing that's no longer true. That is no longer true because you have these specialized chips which with no well-defined ISA, with no contract in between and so independent hardware improvements and software improvements can't happen. So you change your hardware, everything changes. And that means you might have to rewrite a significant portion of your software which is also not only expensive, it slows down the ability to revise your software. Exactly. And so we have to bring that thing back into these specialized chips as well. And one way we are doing this is a lot of these specialized chips, yes they're specialized for an application, but the main reasons why they do better is very universal. So the reason you are able to speed up things is because you parallelize. So you do a lot of work together in parallel. And the second reason things become faster is because you have locality. So accessing some data from the memory is very cost intensive. So very energy intensive, takes a lot of time. So modern computer chips are fast because they get some data from the memory and then they do a lot of work on it. So that data gets reused a lot. And these accelerators also exploit these two principles. So one of our approach is that instead of thinking of these specialized accelerators as like special things each for their own application, can you think of some general abstractions for these? So can you have a common template that with some modifications, you can sort of specialize it for application A, specialize it for application B and specialize it for application C and so on. So you're trying to regain some of that general purpose language that you described how in the old days with CPUs, it was a very fixed language, maybe not fixed, but maybe familiar enough so that you can say, I'm now doing something in parallel and I'm doing it in a local way where I'm not doing a lot of fetches from the memory. And there's enough commonality there that your software can be shielded from all the changes in the hardware that are going on in detail. Exactly. So we call these new systems coarse grained reconfigurable arrays. Okay, wait a minute. That's a big one. Coarse grain reconfigurable array. So just take us through each of those words. Okay. So an array is just a big grid of these units and those units are either computation units or memory units. Okay. So it's a, it's so we can store some data and the compute units can perform some computation on them. Now, because it's a big array, it has this parallelism that we talked about. Lots of things can happen together in that array, right? Reconfigurable because it's some array, but then you can program it to keep up with changing applications. So it's not going to go obsolete very quickly. Right? So you have some re-config. It's not as programmable as your processors, but it's not as not programmable as your very specialized chips. So it's interesting because you're obviously trying to recapture some of that general purpose character without throwing the baby out with the bathwater. You want to make sure it's still fast. It still takes advantage of parallelism and locality, as you mentioned, but you can't make it totally general purpose. So you're, you're skating this fine line. It sounds like exactly. And then coarse grained is because this programmability is not happening at a very fine grain level. So this distinction is from this, another set of devices called FPGAs, which are programmable at the level of gates. That's a lot of programmability and most applications don't need that. So these are programmable at the level of like arithmetic units and things like that. That's a coarser grain than this previous other architecture. We have these coarse grained reconfigurable arrays and depending upon how flexible or specialized these compute units are in these arrays, the same architecture can be closer to FPGAs more programmable, or it can be closer to very specialized chips, right? Depending upon what you put into these compute units. However, this, the way of way of thinking about accelerators like this gives you a nice clean abstraction on which you can build a compiler system because the compiler now has a standard way of thinking about what the hardware is. Right. And a compiler of course is the, is the program that takes the computer program written by the software engineer and turns it into all the hardware instructions that need to be run. And if the hardware changes every time, willy nilly, then the compiler has to be totally rewritten. But if the, if the hardware maintains some order and some predictability, then it may be relatively easier for the compiler to adjust to a new hardware scheme. Exactly. And so one final grain detail about that is that now we are allowing hardware to change by allowing these units in the CGR array to change. But one of the important things is to be able to, can we make the compiler update automatically as this hardware changes? That is the biggest sort of thing that we have going on in this project. And we've actually made that happen. So the compiler can be broken down into several layers and the top layers of that compiler don't have to change very much, but the lowest layer that takes some intermediate representation of your program and converts it to hardware instructions, that layer has to update with every hardware change. And what we have done is by using some tools, formal tools, we are able to specify this hardware in a way that we can generate that last level of compiler automatically without any manual intervention. Right. And so- Huge. Huge. Because then the changes to the hardware for the programmer are irrelevant and they don't have to worry about it as long as your program works, which I'm sure it does because there's a lot of writing on this. That's basically the key point. And I want to tie it a little bit into the application. So this is important because applications previously used to change at a much slower pace. These days with the advances in AI and machine learning, right, everyone is trying to use machine learning to get better performance in their own different domains like speech recognition, image processing, computer vision, things like that. So it's pervasive. And those models are seeing a lot of development. Those machine learning models are like changing every day. Right. And so it's very hard to design specialized hardware when the application itself is changing so quickly. And this whole paradigm allows us to create a hardware software system that can adapt to those changing domains and can allow people to kind of tape out chips very quickly and get the highest performance and sort of track the evolution of applications. You have been listening to the Future of Everything podcast with Russ Altman. I want to remind you that the Future of Everything started out as a radio show on SiriusXM. So you'll hear references to that. Now it is a hundred percent podcast, but we still have access to the great shows that we taped with SiriusXM. There are more than 215 of them and they cover an extraordinary range of topics. If you're enjoying the podcast, please consider subscribing or following so that you can be alerted to every new episode and never be surprised by the future. Maybe tell your friends about it too. Definitely consider rating and reviewing it. That helps us grow, improve, and also spreads the word. You can connect with me on Twitter at rbaltman and with stanfordengineering at stanfordeng."}, "podcast_summary": "In this conversation with Priya Raina, a professor of electrical engineering at Stanford University, they discuss the slowing pace of progress in chip efficiency and the need for chip makers to shift from general-purpose chips to task-specific chips. They highlight the importance of computer chips in everyday life, including the challenges faced by the auto industry due to problems in the supply chain for critical computer chips. Priya explains the current process for creating chips and the problems with that process, such as the complexity of the software tool chain and the time it takes to design, fabricate, and test chips. They discuss the need to make chip design more accessible and the opportunities presented by open-source hardware and electronic design automation (EDA) tools. Priya introduces the concept of coarse-grained reconfigurable arrays, which provide a balance between programmability and specialization in hardware design. They highlight the importance of having a compiler system that can automatically adapt to hardware changes, especially in the context of rapidly changing applications like machine learning. The goal is to create a hardware-software system that can adapt quickly to changing domains and enable the rapid development and deployment of specialized chips.", "podcast_guest": {"name": "Priya Raina", "summary": "Stanford UniversityCannot get guest title..."}, "podcast_highlights": "In this podcast transcript, Professor Priya Raina discusses the slowing pace of progress in chip efficiency and suggests a future where chip makers will need to shift away from general-purpose chips to task-specific chips. She highlights the challenge of relying on a central processing unit (CPU) for general-purpose computing and suggests that specialized accelerators, such as application-specific integrated circuits (ASICs) or hardware accelerators, provide a more efficient alternative. Priya also discusses the current process for creating chips, the problems with the traditional approach, and opportunities to make chip design more accessible and agile. She emphasizes the importance of open-source hardware and the need for open-source EDA (electronic design automation) tools to facilitate innovation and reduce design time. Priya and her group are working on developing coarse-grained reconfigurable arrays, which provide a balance between flexibility and specialization in chip design. These arrays allow for parallel processing and exploit principles like parallelism and locality to improve performance. The goal is to create a hardware-software system that can adapt to changing domains and support faster chip design."}